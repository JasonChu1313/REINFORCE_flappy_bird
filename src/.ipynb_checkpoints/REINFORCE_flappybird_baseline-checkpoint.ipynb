{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import skimage as skimage\n",
    "from skimage import transform, color, exposure\n",
    "from skimage.transform import rotate\n",
    "from skimage.viewer import ImageViewer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "work_space = os.path.dirname(os.getcwd())\n",
    "sys.path.append(work_space+\"/game/\")\n",
    "import wrapped_flappy_bird as game\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import json\n",
    "import os\n",
    "\n",
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.nn.criterion import *\n",
    "from zoo.pipeline.api.keras.models import Sequential\n",
    "from zoo.pipeline.api.keras.layers import Dense, Dropout, Activation,Flatten,Convolution2D\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_RDD(X, y):\n",
    "    return sc.parallelize(X).zip(sc.parallelize(y)).map(\n",
    "            lambda x: Sample.from_ndarray(x[0],x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(advantages, smallEps=1e-8):\n",
    "    return (advantages - advantages.mean())/(advantages.std() + smallEps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_PER_FRAME = 1\n",
    "IMAGE_ROWS, IMAGE_COLS = 80, 80\n",
    "IMAGE_CHANNELS = 4\n",
    "ACTION_SIZE = 2\n",
    "LEARNING_RATE = 1e-4\n",
    "INITIAL_EPSILON = 0.1\n",
    "FINAL_EPSILON = 0.0001\n",
    "EXPLORE = 3000000\n",
    "BATCH_SIZE = 2 # every how many eposides do a parameter update\n",
    "GAMMA = 0.99\n",
    "r_reward_moving_average = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the abstraction of birdagent\n",
    "class BirdAgent:\n",
    "    def __init__(self, state_size, action_size, gamma=0.95):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = gamma\n",
    "        self.model = self._build_mode()\n",
    "\n",
    "    def _build_mode(self):\n",
    "        print(\"Now we build the model\")\n",
    "        model = Sequential()\n",
    "        model.add(Convolution2D(32, 8, 8, subsample=(4, 4), border_mode='same',\n",
    "                                input_shape=(IMAGE_ROWS, IMAGE_COLS, IMAGE_CHANNELS)))  # 80*80*4\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Convolution2D(64, 4, 4, subsample=(2, 2), border_mode='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Convolution2D(64, 3, 3, subsample=(1, 1), border_mode='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(2))\n",
    "        # get the 1 * 2 output represent each action's probability\n",
    "        model.add(Activation('softmax'))\n",
    "        return model\n",
    "\n",
    "    # sample the action from the predict of the model\n",
    "    def action_sampler(self, out):\n",
    "        # return 1 if result > np.random.random() else 0\n",
    "        return np.random.choice([0, 1], p=out)\n",
    "\n",
    "    def act(self, state, epsilon = 0.01):\n",
    "        explore = False\n",
    "        random_num = random.random()\n",
    "        action = np.zeros(2)\n",
    "        if random_num <= epsilon:\n",
    "            explore = True\n",
    "            # ramdomly select an action\n",
    "            action_index = random.randrange(ACTION_SIZE)\n",
    "            action[action_index] = 1\n",
    "            print(\"*********** Random Action *********** : \", action_index)\n",
    "            return action,action,explore\n",
    "\n",
    "        else:\n",
    "            if isinstance(state, np.ndarray):\n",
    "                features = to_sample_rdd(state, np.zeros([state.shape[0]]))\n",
    "            out = self.model.predict(features)\n",
    "            #print(\"out type\",out)\n",
    "            out = out.collect()\n",
    "            \n",
    "            if self.action_sampler(np.squeeze(out)) == 0:\n",
    "                action[0] = 1\n",
    "            else:\n",
    "                action[1] = 1\n",
    "            return np.squeeze(out),action,explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sample_rdd(x, y, numSlices=None):\n",
    "    \"\"\"\n",
    "    Conver x and y into RDD[Sample]\n",
    "    :param x: ndarray and the first dimension should be batch\n",
    "    :param y: ndarray and the first dimension should be batch\n",
    "    :param numSlices:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sc = get_spark_context()\n",
    "    from bigdl.util.common import Sample\n",
    "    x_rdd = sc.parallelize(x, numSlices)\n",
    "    y_rdd = sc.parallelize(y, numSlices)\n",
    "    return x_rdd.zip(y_rdd).map(lambda item: Sample.from_ndarray(item[0], item[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the abstraction of rollouts\n",
    "class RollOuts():\n",
    "    def __init__(self, next_states, actions, rewards, logprobs, gradients, total_steps):\n",
    "        self.next_states = next_states\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        self.steps = total_steps\n",
    "        self.logprobs = logprobs\n",
    "        self.gradients = gradients\n",
    "        self.total_rewards = np.sum(rewards)\n",
    "\n",
    "\n",
    "    def get_summary(self):\n",
    "        return {\" total_reward \": self.total_rewards,\n",
    "                \" total_steps \": self.steps}\n",
    "\n",
    "\n",
    "    def prepare_target(self):\n",
    "        result = []\n",
    "        for action, adv in list(zip(self.actions, self.advs)):\n",
    "            adv = adv * action\n",
    "            print('action and advantages : ',action, adv)\n",
    "            result.append(adv)\n",
    "        return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the abstraction of experience memory\n",
    "class ExperienceStroe:\n",
    "    def __init__(self):\n",
    "        self.rollouts = []\n",
    "\n",
    "    def add_rollout(self, next_states, actions, rewards,logprobs, gradients, total_steps):\n",
    "        self.rollouts.append(RollOuts(next_states, actions, rewards, logprobs, gradients,total_steps))\n",
    "\n",
    "    def num_experiences(self):\n",
    "        return len(self.rollouts)\n",
    "\n",
    "    def get_range(self, start, end):\n",
    "        return self.rollouts[start:end]\n",
    "\n",
    "    def reset(self):\n",
    "        self.rollouts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_summary(rollouts, records, verbose=True):\n",
    "    rollout_rewards = np.array([rollout.total_rewards for rollout in rollouts])\n",
    "    print(\"reward mean %s\" % (rollout_rewards.mean()))\n",
    "    print(\"reward std %s\" % (rollout_rewards.std()))\n",
    "    print(\"reward max %s\" % (rollout_rewards.max()))\n",
    "    records.append([rollout_rewards.mean(), rollout_rewards.std()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate running reward\n",
    "def cal_reward(rollouts, gamma, discounted=False):\n",
    "    for rollout in rollouts:\n",
    "        rewards = rollout.rewards\n",
    "        r_reward = []\n",
    "        running_reward = 0\n",
    "        for reward in rewards[::-1]:\n",
    "            # discounted reward\n",
    "            # reward = (-1) * reward\n",
    "            if discounted == True:\n",
    "                running_reward = gamma * running_reward + reward\n",
    "            else:\n",
    "                running_reward = running_reward + reward\n",
    "            r_reward.append(running_reward)\n",
    "        rollout.r_rewards = np.squeeze(np.array(np.vstack(r_reward[::-1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_advantage_moving_average(rollouts):\n",
    "    r_reward_moving_average = 0\n",
    "    step = 0\n",
    "    for rollout in rollouts:\n",
    "        advs = np.zeros([rollout.rewards.shape[0]])\n",
    "        i = 0\n",
    "        for r_reward in rollout.r_rewards:\n",
    "            # calculate moving average\n",
    "            r_reward_moving_average = (1-0.9) * r_reward_moving_average + 0.1 * r_reward\n",
    "            # correct the bias\n",
    "            print (\"lalla\",1-np.power(0.9, step))\n",
    "            advs[i] = r_reward - r_reward_moving_average\n",
    "            i += 1\n",
    "            step += 1\n",
    "            rollout.advs = advs       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the advantage = reward - expected reward at this time step\n",
    "def cal_advantage(rollouts):\n",
    "    max_steps = max(rollout.rewards.shape[0] for rollout in rollouts)\n",
    "    for rollout in rollouts:\n",
    "        rollout.r_rewards = np.pad(rollout.r_rewards, (0, max_steps - rollout.r_rewards.shape[0]),'constant')\n",
    "    baselines = np.mean(np.vstack([rollout.r_rewards for rollout in rollouts]), axis=0)\n",
    "    for rollout in rollouts:\n",
    "        rollout.advs = rollout.r_rewards - baselines\n",
    "        rollout.advs = rollout.advs[:len(rollout.r_rewards)]\n",
    "        rollout.r_rewards = rollout.r_rewards[:len(rollout.rewards)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(agent, render=False):\n",
    "    # 1. initialize\n",
    "    # open up a game state to communicate with emulator\n",
    "    game_state = game.GameState()\n",
    "    # used to save the actions, should be a 1*2 nparray and sum(action) should be 1\n",
    "    actions = np.zeros([1,2])\n",
    "    # used to save the rewards\n",
    "    rewards = np.array([])\n",
    "    # get the first state by doing nothing and preprocess the image to 80x80x4\n",
    "  \n",
    "\n",
    "    gradients = np.zeros([1, 2])\n",
    "    # should be (1*2) nparray and each col represent the probability to chose that action\n",
    "    logprobs = np.array([1, 2])\n",
    "\n",
    "\n",
    "    do_nothing = np.zeros(ACTION_SIZE)\n",
    "    do_nothing[0] = 1\n",
    "    x_t, r_0, terminal = game_state.frame_step(do_nothing)\n",
    "\n",
    "    # preprocess the first frame\n",
    "    x_t = skimage.color.rgb2gray(x_t)\n",
    "    x_t = skimage.transform.resize(x_t, (80, 80))\n",
    "    x_t = skimage.exposure.rescale_intensity(x_t, out_range=(0, 255))\n",
    "\n",
    "    # rescale to 0-1\n",
    "    x_t = x_t / 255.0\n",
    "\n",
    "    # 80 * 80 * 4\n",
    "    state = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "    print (state.shape)\n",
    "    # In Keras, need to reshape 1 * 80 * 80 * 4\n",
    "    state = state.reshape(1, state.shape[0], state.shape[1], state.shape[2])  # 1*80*80*4\n",
    "    # the observations will be batch_size * 80 * 80 * 4\n",
    "    observations = np.zeros([1, state.shape[1], state.shape[2], state.shape[3]])\n",
    "\n",
    "    for step in range(1, 500):\n",
    "        # next state's shape, should be (1*80*80*4)\n",
    "        \n",
    "        # get the state list\n",
    "        observations = np.vstack((observations, state))\n",
    "        # predict the action\n",
    "\n",
    "        logprob,action,explore = agent.act(state)\n",
    "        \n",
    "        \n",
    "        actions = np.vstack((actions, action))\n",
    "        logprobs = np.vstack((logprobs ,logprob))\n",
    "        gradients = np.vstack((gradients , action.astype('float32') - logprob))\n",
    "        # use the predicted action to determine the next state\n",
    "        x_t1_colored, reward, terminal = game_state.frame_step(action)\n",
    "        print ('reward ',reward)\n",
    "        # rgb to gray and rescale\n",
    "        x_t1 = skimage.color.rgb2gray(x_t1_colored)\n",
    "        x_t1 = skimage.transform.resize(x_t1, (80, 80))\n",
    "        x_t1 = skimage.exposure.rescale_intensity(x_t1, out_range=(0, 255))\n",
    "        # rescale to 0-1\n",
    "        x_t1 = x_t1 / 255\n",
    "        # update state\n",
    "        x_t1 = x_t1.reshape(1,x_t1.shape[0],x_t1.shape[1],1)\n",
    "        state = np.append(x_t1,state[:,:,:,:3],axis=3)\n",
    "        rewards = np.append(rewards, reward)\n",
    "\n",
    "        print('living steps : ',observations.shape[0])\n",
    "        # print('state size ',state.shape)\n",
    "        # print('action size : ',action.shape)\n",
    "        # print('action : ',action)\n",
    "        # print('reward : ',reward)\n",
    "        # print('terminal : ',terminal)\n",
    "        if terminal or step == 498:\n",
    "            break\n",
    "    return observations[1:], actions[1:], rewards , logprobs[1:], gradients[1:], step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_n_games(agent, history, n=4, verbose=True):\n",
    "    start_eps = history.num_experiences()\n",
    "    total_step = 0\n",
    "    for i in range(n):\n",
    "        observations, actions, rewards, logprobs, gradients, step = play_game(agent=agent)\n",
    "        history.add_rollout(observations, actions, rewards, logprobs, gradients, step)\n",
    "        total_step += step\n",
    "    end_eps = history.num_experiences()\n",
    "    return start_eps, end_eps, total_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(agent, rollouts):\n",
    "    cal_reward(rollouts, agent.gamma)\n",
    "    cal_advantage_moving_average\n",
    "    #cal_advantage(rollouts)\n",
    "    X_batch = np.zeros([1, 80, 80, 4])\n",
    "    Y_batch = np.array([0,0])\n",
    "    for rollout in rollouts:\n",
    "        X_batch = np.vstack((X_batch, rollout.next_states))\n",
    "        # Y_batch: the label of the training should be [[action,adv]]\n",
    "        Y_batch = np.vstack((Y_batch, rollout.prepare_target()))\n",
    "    X_batch = X_batch[1:]\n",
    "    Y_batch = Y_batch[1:]\n",
    "    Y_batch[:,1] = normalize(Y_batch[:,1])\n",
    "\n",
    "    # Y_batch[:, 1] = normalize(y_batch[:, 1])\n",
    "    # prepare to train the model\n",
    "    print('X_batch size : ',X_batch.shape)\n",
    "    print('Y_batch size : ',Y_batch.shape)\n",
    "    rdd_sample = to_RDD(X_batch, Y_batch)\n",
    "    batch_size = X_batch.shape[0] - X_batch.shape[0]%8\n",
    "    print (\"using batch_size = \",batch_size)\n",
    "    \n",
    "    optimizer = Optimizer(model=agent.model,\n",
    "                                  training_rdd=rdd_sample,\n",
    "                                  criterion=PGCriterion(),\n",
    "                                  optim_method= RMSprop(learningrate=0.005),\n",
    "                                  end_trigger=MaxIteration(1),\n",
    "                                  batch_size=batch_size)\n",
    "    #else:\n",
    "        #optimizer.set_traindata(training_rdd=rdd_sample, batch_size=batch_size)\n",
    "        #optimizer.set_criterion(RFPGCriterion())\n",
    "    agent.model = optimizer.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we build the model\n",
      "creating: createZooKerasSequential\n",
      "creating: createZooKerasConvolution2D\n",
      "creating: createZooKerasActivation\n",
      "creating: createZooKerasConvolution2D\n",
      "creating: createZooKerasActivation\n",
      "creating: createZooKerasConvolution2D\n",
      "creating: createZooKerasActivation\n",
      "creating: createZooKerasFlatten\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasActivation\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasActivation\n",
      "(80, 80, 4)\n",
      "reward  0.1\n",
      "living steps :  2\n",
      "reward  0.1\n",
      "living steps :  3\n",
      "reward  0.1\n",
      "living steps :  4\n",
      "reward  0.1\n",
      "living steps :  5\n",
      "reward  0.1\n",
      "living steps :  6\n",
      "reward  0.1\n",
      "living steps :  7\n",
      "reward  0.1\n",
      "living steps :  8\n",
      "reward  0.1\n",
      "living steps :  9\n",
      "reward  0.1\n",
      "living steps :  10\n",
      "reward  0.1\n",
      "living steps :  11\n",
      "reward  0.1\n",
      "living steps :  12\n",
      "reward  0.1\n",
      "living steps :  13\n",
      "reward  0.1\n",
      "living steps :  14\n",
      "reward  0.1\n",
      "living steps :  15\n",
      "reward  0.1\n",
      "living steps :  16\n",
      "reward  0.1\n",
      "living steps :  17\n",
      "reward  0.1\n",
      "living steps :  18\n",
      "reward  0.1\n",
      "living steps :  19\n",
      "reward  0.1\n",
      "living steps :  20\n",
      "reward  0.1\n",
      "living steps :  21\n",
      "reward  0.1\n",
      "living steps :  22\n",
      "reward  0.1\n",
      "living steps :  23\n",
      "reward  0.1\n",
      "living steps :  24\n",
      "reward  0.1\n",
      "living steps :  25\n",
      "reward  0.1\n",
      "living steps :  26\n",
      "reward  0.1\n",
      "living steps :  27\n",
      "reward  0.1\n",
      "living steps :  28\n",
      "reward  0.1\n",
      "living steps :  29\n",
      "reward  0.1\n",
      "living steps :  30\n",
      "reward  0.1\n",
      "living steps :  31\n",
      "reward  0.1\n",
      "living steps :  32\n",
      "reward  0.1\n",
      "living steps :  33\n",
      "reward  0.1\n",
      "living steps :  34\n",
      "reward  0.1\n",
      "living steps :  35\n",
      "reward  0.1\n",
      "living steps :  36\n",
      "reward  0.1\n",
      "living steps :  37\n",
      "reward  0.1\n",
      "living steps :  38\n",
      "reward  0.1\n",
      "living steps :  39\n",
      "reward  0.1\n",
      "living steps :  40\n",
      "*********** Random Action *********** :  1\n",
      "reward  0.1\n",
      "living steps :  41\n",
      "reward  0.1\n",
      "living steps :  42\n",
      "reward  0.1\n",
      "living steps :  43\n",
      "reward  0.1\n",
      "living steps :  44\n",
      "reward  0.1\n",
      "living steps :  45\n",
      "reward  0.1\n",
      "living steps :  46\n",
      "reward  0.1\n",
      "living steps :  47\n",
      "reward  0.1\n",
      "living steps :  48\n",
      "reward  0.1\n",
      "living steps :  49\n",
      "reward  -1\n",
      "living steps :  50\n",
      "(80, 80, 4)\n",
      "reward  0.1\n",
      "living steps :  2\n",
      "reward  0.1\n",
      "living steps :  3\n",
      "reward  0.1\n",
      "living steps :  4\n",
      "reward  0.1\n",
      "living steps :  5\n",
      "reward  0.1\n",
      "living steps :  6\n",
      "reward  0.1\n",
      "living steps :  7\n",
      "reward  0.1\n",
      "living steps :  8\n",
      "reward  0.1\n",
      "living steps :  9\n",
      "reward  0.1\n",
      "living steps :  10\n",
      "reward  0.1\n",
      "living steps :  11\n",
      "reward  0.1\n",
      "living steps :  12\n",
      "reward  0.1\n",
      "living steps :  13\n",
      "reward  0.1\n",
      "living steps :  14\n",
      "reward  0.1\n",
      "living steps :  15\n",
      "reward  0.1\n",
      "living steps :  16\n",
      "reward  0.1\n",
      "living steps :  17\n",
      "reward  0.1\n",
      "living steps :  18\n",
      "reward  0.1\n",
      "living steps :  19\n",
      "reward  0.1\n",
      "living steps :  20\n",
      "reward  0.1\n",
      "living steps :  21\n",
      "reward  0.1\n",
      "living steps :  22\n",
      "reward  0.1\n",
      "living steps :  23\n",
      "reward  0.1\n",
      "living steps :  24\n",
      "reward  0.1\n",
      "living steps :  25\n",
      "reward  0.1\n",
      "living steps :  26\n",
      "reward  0.1\n",
      "living steps :  27\n",
      "reward  0.1\n",
      "living steps :  28\n",
      "reward  0.1\n",
      "living steps :  29\n",
      "reward  0.1\n",
      "living steps :  30\n",
      "reward  0.1\n",
      "living steps :  31\n",
      "reward  0.1\n",
      "living steps :  32\n",
      "reward  0.1\n",
      "living steps :  33\n",
      "reward  0.1\n",
      "living steps :  34\n",
      "reward  0.1\n",
      "living steps :  35\n",
      "reward  0.1\n",
      "living steps :  36\n",
      "reward  0.1\n",
      "living steps :  37\n",
      "reward  0.1\n",
      "living steps :  38\n",
      "reward  0.1\n",
      "living steps :  39\n",
      "reward  0.1\n",
      "living steps :  40\n",
      "reward  0.1\n",
      "living steps :  41\n",
      "reward  0.1\n",
      "living steps :  42\n",
      "reward  0.1\n",
      "living steps :  43\n",
      "reward  0.1\n",
      "living steps :  44\n",
      "reward  0.1\n",
      "living steps :  45\n",
      "reward  0.1\n",
      "living steps :  46\n",
      "reward  0.1\n",
      "living steps :  47\n",
      "reward  0.1\n",
      "living steps :  48\n",
      "reward  0.1\n",
      "living steps :  49\n",
      "reward  -1\n",
      "living steps :  50\n",
      "reward mean 3.8\n",
      "reward std 0.0\n",
      "reward max 3.8\n",
      "used in training:  0 - 2\n",
      "num of total steps in training  98\n",
      "training begin\n",
      "action and advantages :  [0. 1.] [-0.   -3.42]\n",
      "action and advantages :  [0. 1.] [-0.    -3.292]\n",
      "action and advantages :  [1. 0.] [-3.1992 -0.    ]\n",
      "action and advantages :  [1. 0.] [-3.10992 -0.     ]\n",
      "action and advantages :  [0. 1.] [-0.       -3.020992]\n",
      "action and advantages :  [1. 0.] [-2.9320992 -0.       ]\n",
      "action and advantages :  [0. 1.] [-0.         -2.84320992]\n",
      "action and advantages :  [0. 1.] [-0.         -2.75432099]\n",
      "action and advantages :  [1. 0.] [-2.6654321 -0.       ]\n",
      "action and advantages :  [1. 0.] [-2.57654321 -0.        ]\n",
      "action and advantages :  [1. 0.] [-2.48765432 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -2.39876543]\n",
      "action and advantages :  [1. 0.] [-2.30987654 -0.        ]\n",
      "action and advantages :  [1. 0.] [-2.22098765 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -2.13209877]\n",
      "action and advantages :  [0. 1.] [-0.         -2.04320988]\n",
      "action and advantages :  [0. 1.] [-0.         -1.95432099]\n",
      "action and advantages :  [1. 0.] [-1.8654321 -0.       ]\n",
      "action and advantages :  [0. 1.] [-0.         -1.77654321]\n",
      "action and advantages :  [0. 1.] [-0.         -1.68765432]\n",
      "action and advantages :  [1. 0.] [-1.59876543 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -1.50987654]\n",
      "action and advantages :  [0. 1.] [-0.         -1.42098765]\n",
      "action and advantages :  [0. 1.] [-0.         -1.33209877]\n",
      "action and advantages :  [0. 1.] [-0.         -1.24320988]\n",
      "action and advantages :  [1. 0.] [-1.15432099 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.        -1.0654321]\n",
      "action and advantages :  [0. 1.] [-0.         -0.97654321]\n",
      "action and advantages :  [1. 0.] [-0.88765432 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.79876543]\n",
      "action and advantages :  [0. 1.] [-0.         -0.70987654]\n",
      "action and advantages :  [0. 1.] [-0.         -0.62098765]\n",
      "action and advantages :  [0. 1.] [-0.         -0.53209877]\n",
      "action and advantages :  [0. 1.] [-0.         -0.44320988]\n",
      "action and advantages :  [0. 1.] [-0.         -0.35432099]\n",
      "action and advantages :  [1. 0.] [-0.2654321 -0.       ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.17654321]\n",
      "action and advantages :  [0. 1.] [-0.         -0.08765432]\n",
      "action and advantages :  [1. 0.] [0.00123457 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         0.09012346]\n",
      "action and advantages :  [1. 0.] [0.17901235 0.        ]\n",
      "action and advantages :  [1. 0.] [0.26790123 0.        ]\n",
      "action and advantages :  [1. 0.] [0.35679012 0.        ]\n",
      "action and advantages :  [1. 0.] [0.44567901 0.        ]\n",
      "action and advantages :  [0. 1.] [0.        0.5345679]\n",
      "action and advantages :  [1. 0.] [0.62345679 0.        ]\n",
      "action and advantages :  [1. 0.] [0.71234568 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         0.80123457]\n",
      "action and advantages :  [0. 1.] [0.         0.89012346]\n",
      "action and advantages :  [1. 0.] [-3.43098765 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -3.29309877]\n",
      "action and advantages :  [0. 1.] [-0.         -3.19930988]\n",
      "action and advantages :  [0. 1.] [-0.         -3.10993099]\n",
      "action and advantages :  [1. 0.] [-3.0209931 -0.       ]\n",
      "action and advantages :  [1. 0.] [-2.93209931 -0.        ]\n",
      "action and advantages :  [1. 0.] [-2.84320993 -0.        ]\n",
      "action and advantages :  [1. 0.] [-2.75432099 -0.        ]\n",
      "action and advantages :  [1. 0.] [-2.6654321 -0.       ]\n",
      "action and advantages :  [1. 0.] [-2.57654321 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -2.48765432]\n",
      "action and advantages :  [1. 0.] [-2.39876543 -0.        ]\n",
      "action and advantages :  [1. 0.] [-2.30987654 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -2.22098765]\n",
      "action and advantages :  [1. 0.] [-2.13209877 -0.        ]\n",
      "action and advantages :  [1. 0.] [-2.04320988 -0.        ]\n",
      "action and advantages :  [1. 0.] [-1.95432099 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.        -1.8654321]\n",
      "action and advantages :  [1. 0.] [-1.77654321 -0.        ]\n",
      "action and advantages :  [1. 0.] [-1.68765432 -0.        ]\n",
      "action and advantages :  [1. 0.] [-1.59876543 -0.        ]\n",
      "action and advantages :  [1. 0.] [-1.50987654 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -1.42098765]\n",
      "action and advantages :  [1. 0.] [-1.33209877 -0.        ]\n",
      "action and advantages :  [1. 0.] [-1.24320988 -0.        ]\n",
      "action and advantages :  [1. 0.] [-1.15432099 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.        -1.0654321]\n",
      "action and advantages :  [1. 0.] [-0.97654321 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.88765432]\n",
      "action and advantages :  [0. 1.] [-0.         -0.79876543]\n",
      "action and advantages :  [1. 0.] [-0.70987654 -0.        ]\n",
      "action and advantages :  [1. 0.] [-0.62098765 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.53209877]\n",
      "action and advantages :  [0. 1.] [-0.         -0.44320988]\n",
      "action and advantages :  [0. 1.] [-0.         -0.35432099]\n",
      "action and advantages :  [1. 0.] [-0.2654321 -0.       ]\n",
      "action and advantages :  [1. 0.] [-0.17654321 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.08765432]\n",
      "action and advantages :  [1. 0.] [0.00123457 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         0.09012346]\n",
      "action and advantages :  [0. 1.] [0.         0.17901235]\n",
      "action and advantages :  [0. 1.] [0.         0.26790123]\n",
      "action and advantages :  [1. 0.] [0.35679012 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         0.44567901]\n",
      "action and advantages :  [0. 1.] [0.        0.5345679]\n",
      "action and advantages :  [0. 1.] [0.         0.62345679]\n",
      "action and advantages :  [1. 0.] [0.71234568 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         0.80123457]\n",
      "action and advantages :  [1. 0.] [0.89012346 0.        ]\n",
      "X_batch size :  (98, 80, 80, 4)\n",
      "Y_batch size :  (98, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using batch_size =  96\n",
      "creating: createPGCriterion\n",
      "creating: createRMSprop\n",
      "creating: createMaxIteration\n",
      "creating: createDistriOptimizer\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o21428.optimize.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 985.0 failed 1 times, most recent failure: Lost task 0.0 in stage 985.0 (TID 7051, localhost): java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$5$$anonfun$8.apply(DistriOptimizer.scala:262)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$5$$anonfun$8.apply(DistriOptimizer.scala:262)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$5.apply(DistriOptimizer.scala:262)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$5.apply(DistriOptimizer.scala:202)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:312)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:914)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$5$$anonfun$8.apply(DistriOptimizer.scala:262)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$5$$anonfun$8.apply(DistriOptimizer.scala:262)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$5.apply(DistriOptimizer.scala:262)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$5.apply(DistriOptimizer.scala:202)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: java.lang.OutOfMemoryError: Java heap space\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-1213f3f6353c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mrollouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training begin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrollouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mend_of_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mexe_time_game_play\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend_of_play\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_of_play\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-e74dba537797>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(agent, rollouts)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m#optimizer.set_traindata(training_rdd=rdd_sample, batch_size=batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m#optimizer.set_criterion(RFPGCriterion())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/spark-dc653705-f757-44f5-9e1c-35bb624f405c/userFiles-30f913a6-5b0a-4bc8-a464-c020fde89f8e/analytics-zoo-0.1.0-SNAPSHOT-python-api.zip/bigdl/optim/optimizer.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0mDo\u001b[0m \u001b[0man\u001b[0m \u001b[0moptimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \"\"\"\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0mjmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/spark-dc653705-f757-44f5-9e1c-35bb624f405c/userFiles-30f913a6-5b0a-4bc8-a464-c020fde89f8e/analytics-zoo-0.1.0-SNAPSHOT-python-api.zip/bigdl/util/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(func, *args)\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0mgateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/spark/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/spark/spark-1.6.0-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/spark/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    307\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o21428.optimize.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 985.0 failed 1 times, most recent failure: Lost task 0.0 in stage 985.0 (TID 7051, localhost): java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$5$$anonfun$8.apply(DistriOptimizer.scala:262)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$5$$anonfun$8.apply(DistriOptimizer.scala:262)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$5.apply(DistriOptimizer.scala:262)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$5.apply(DistriOptimizer.scala:202)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:312)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:914)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$5$$anonfun$8.apply(DistriOptimizer.scala:262)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$5$$anonfun$8.apply(DistriOptimizer.scala:262)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$5.apply(DistriOptimizer.scala:262)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$5.apply(DistriOptimizer.scala:202)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: java.lang.OutOfMemoryError: Java heap space\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "state_size = (1, 80, 80, 4)\n",
    "action_size = ACTION_SIZE\n",
    "agent = BirdAgent(state_size, action_size)\n",
    "history = ExperienceStroe()\n",
    "record = []\n",
    "exe_times = []\n",
    "history.reset()\n",
    "train_start = 0\n",
    "played_steps = 0\n",
    "start_of_play = timeit.default_timer()\n",
    "for i in range(100):\n",
    "    # history is a list of rollouts\n",
    "    start_eps, end_eps, steps = play_n_games(agent, history, n=BATCH_SIZE)\n",
    "    played_steps += steps\n",
    "    train_end = end_eps\n",
    "    end_of_play = timeit.default_timer()\n",
    "    print\n",
    "    \"*************************\"\n",
    "    rollouts = history.get_range(start_eps, end_eps)\n",
    "    stats_summary(rollouts, record)\n",
    "    if (played_steps > BATCH_SIZE * 34):\n",
    "        print(\"used in training: \", start_eps, \"-\", end_eps)\n",
    "        print(\"num of total steps in training \", played_steps)\n",
    "        start_of_train = timeit.default_timer()\n",
    "        rollouts = history.get_range(train_start, train_end)\n",
    "        print(\"training begin\")\n",
    "        learn(agent, rollouts)\n",
    "        end_of_train = timeit.default_timer()\n",
    "        exe_time_game_play = end_of_play - start_of_play\n",
    "        train_time_game_paly = end_of_train - start_of_train\n",
    "        exe_times.append([exe_time_game_play, train_time_game_paly])\n",
    "        \n",
    "        played_steps = 0\n",
    "        start_of_play = timeit.default_timer()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
